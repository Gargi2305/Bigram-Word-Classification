{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGoTYC3f39Z4qte8k21AID",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gargi2305/Bigram-Word-Classification/blob/main/word%20classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7BwXJBrIwxUD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "class WordBigramModel:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = None\n",
        "        self.classifier = None\n",
        "        self.words = None\n",
        "\n",
        "    def fit(self, words):\n",
        "        self.words = words\n",
        "        self.vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
        "        X = self.vectorizer.fit_transform(words)\n",
        "\n",
        "        # Find the minimum class size\n",
        "        unique, counts = np.unique(words, return_counts=True)\n",
        "        min_class_size = min(counts)\n",
        "\n",
        "        # Ensure n_splits does not exceed min_class_size and is at least 2\n",
        "        if min_class_size >= 2:\n",
        "            param_grid = {'max_depth': [10, 20, 50, 100, None], 'min_samples_leaf': [1, 2, 5, 10]}\n",
        "            grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=min(5, min_class_size))\n",
        "            grid_search.fit(X, words)\n",
        "            self.classifier = grid_search.best_estimator_\n",
        "        else:\n",
        "            self.classifier = DecisionTreeClassifier(max_depth=400, min_samples_leaf=5)\n",
        "            self.classifier.fit(X, words)\n",
        "\n",
        "    def predict(self, bigram_list, test_word):\n",
        "        match_scores = []\n",
        "\n",
        "        for word in self.words:\n",
        "            score = self.calculate_match_score(bigram_list, word)\n",
        "            if score > 0:  # Only consider words with a positive match score\n",
        "                match_scores.append((word, score))\n",
        "\n",
        "        # Sort by score in descending order, then by word length for closer matches, and alphabetically for stability\n",
        "        match_scores.sort(key=lambda x: (-x[1], len(x[0]), x[0]))\n",
        "\n",
        "        # Determine the threshold score by considering a margin below the highest score\n",
        "        if match_scores:\n",
        "            highest_score = match_scores[0][1]\n",
        "            threshold_score = highest_score * 0.7  # Example: Use 70% of the highest score as threshold\n",
        "        else:\n",
        "            threshold_score = 0\n",
        "\n",
        "        # Select words with scores above the threshold, limiting to a maximum of 5 guesses\n",
        "        best_matches = [word for word, score in match_scores if score >= threshold_score][:5]\n",
        "\n",
        "        # Ensure all words with the same top score are considered as potential correct predictions\n",
        "        top_score_words = [word for word, score in match_scores if score == highest_score]\n",
        "\n",
        "        # Calculate precision score (not used in main function)\n",
        "        precision_score = 1 / len(top_score_words) if test_word in top_score_words else 0\n",
        "\n",
        "        return best_matches[:max(2, len(best_matches))]\n",
        "\n",
        "    def calculate_match_score(self, bigram_list, word):\n",
        "        score = 0\n",
        "        word_bigrams = [word[i:i+2] for i in range(len(word)-1)]  # Generate bigrams from the word\n",
        "        bigram_sequence_score = sum(1 for i, bigram in enumerate(bigram_list) if bigram in word_bigrams[i:i+1])\n",
        "        if bigram_sequence_score == len(bigram_list):\n",
        "            score += bigram_sequence_score * 2  # Double score for complete sequence match\n",
        "        else:\n",
        "            score += sum(word.count(bigram) for bigram in bigram_list)  # Fallback to count-based scoring\n",
        "        return score\n",
        "\n",
        "def my_fit(words):\n",
        "    model = WordBigramModel()\n",
        "    model.fit(words)\n",
        "    return model\n",
        "\n",
        "def my_predict(model, bigram_list):\n",
        "    return model.predict(bigram_list, \"\")  # Pass an empty string or any placeholder as test_word\n",
        "\n"
      ]
    }
  ]
}